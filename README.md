# Proyecto de Scraping Web con SQL Server y RabbitMQ

Este proyecto implementa un sistema de scraping web distribuido que utiliza RabbitMQ para la gestiÃ³n de tareas y SQL Server para el almacenamiento de datos.

## ğŸ“ Estructura del Proyecto

```
scraping_web/
â”œâ”€â”€ src/                          # CÃ³digo fuente principal
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                 # ConfiguraciÃ³n y variables de entorno
â”‚   â”œâ”€â”€ database.py               # GestiÃ³n de conexiÃ³n a SQL Server externo
â”‚   â”œâ”€â”€ rabbitmq_client.py        # Cliente para RabbitMQ externo
â”‚   â”œâ”€â”€ scraper.py                # Motor de scraping web
â”‚   â””â”€â”€ scraping_worker.py        # Worker principal que coordina todo
â”œâ”€â”€ run_production_worker.py      # Worker de producciÃ³n principal (SIEMPRE ACTIVO)
â”œâ”€â”€ requirements.txt              # Dependencias de Python
â”œâ”€â”€ config.env.example            # Ejemplo de configuraciÃ³n local
â”œâ”€â”€ docker.env.example            # Ejemplo de configuraciÃ³n Docker
â”œâ”€â”€ docker-compose.yml            # ConfiguraciÃ³n Docker (solo app Python)
â”œâ”€â”€ Dockerfile                    # Dockerfile para containerizaciÃ³n
â”œâ”€â”€ README.md                     # DocumentaciÃ³n completa
â””â”€â”€ .gitignore                    # Archivos a ignorar en Git
```

## ğŸš€ CaracterÃ­sticas

- **Scraping Web**: ExtracciÃ³n de datos usando requests/BeautifulSoup y Selenium
- **Cola de Mensajes**: RabbitMQ para gestionar tareas de scraping de forma asÃ­ncrona
- **Base de Datos**: SQL Server para almacenar resultados del scraping
- **Escalabilidad**: Arquitectura distribuida que permite mÃºltiples workers
- **ConfiguraciÃ³n Flexible**: Variables de entorno para personalizar el comportamiento
- **Logging Completo**: Registro detallado de todas las operaciones

## ğŸ“‹ Requisitos Previos

### Software Necesario

1. **Python 3.8+**
2. **SQL Server** (servicio externo en otro proyecto)
3. **RabbitMQ Server** (servicio externo en otro proyecto)
4. **Microsoft Edge** (recomendado) o **Chrome/Chromium** (para Selenium)

### Servicios Externos Requeridos

- **SQL Server**: Debe estar ejecutÃ¡ndose en otro proyecto/contenedor
- **RabbitMQ**: Debe estar ejecutÃ¡ndose en otro proyecto/contenedor con usuario `admin` y contraseÃ±a `admin123`

### Drivers de SQL Server

Para Windows, instala el driver ODBC:
- [Microsoft ODBC Driver 17 for SQL Server](https://docs.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server)

### AutenticaciÃ³n de Windows

El proyecto estÃ¡ configurado para usar **autenticaciÃ³n de Windows** (Trusted Connection) con SQL Server:
- **Instancia**: `localhost\MSSQLSERVER01`
- **AutenticaciÃ³n**: Windows (sin usuario/contraseÃ±a)
- **Permisos**: El usuario de Windows debe tener acceso a la base de datos

## ğŸ› ï¸ InstalaciÃ³n

1. **Clonar el repositorio**
   ```bash
   git clone <tu-repositorio>
   cd scraping_web
   ```

2. **Crear entorno virtual**
   ```bash
   python -m venv venv
   
   # Windows
   venv\Scripts\activate
   
   # Linux/Mac
   source venv/bin/activate
   ```

3. **Instalar dependencias**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configurar variables de entorno**
   ```bash
   # Copiar archivo de ejemplo
   copy config.env.example .env
   
   # Editar .env con tus configuraciones
   ```

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno (.env)

```env
# ConfiguraciÃ³n de SQL Server (servicio externo)
SQL_SERVER_HOST=localhost\MSSQLSERVER01
SQL_SERVER_PORT=1433
SQL_SERVER_DATABASE=scraping_db
SQL_SERVER_USERNAME=
SQL_SERVER_PASSWORD=
SQL_SERVER_TRUSTED_CONNECTION=yes

# ConfiguraciÃ³n de RabbitMQ (servicio externo)
RABBITMQ_HOST=rabbitmq_externo
RABBITMQ_PORT=5672
RABBITMQ_USERNAME=admin
RABBITMQ_PASSWORD=admin123
RABBITMQ_QUEUE=scraping_tasks
RABBITMQ_EXCHANGE=aseguradora_exchange

# ConfiguraciÃ³n de la aplicaciÃ³n
LOG_LEVEL=INFO
SCRAPING_DELAY=2
MAX_RETRIES=3
```

### Servicios Externos

1. **SQL Server**: Debe estar ejecutÃ¡ndose en `localhost\MSSQLSERVER01`
   - **AutenticaciÃ³n**: Windows (Trusted Connection)
   - **Base de datos**: Crear `scraping_db`
   ```sql
   CREATE DATABASE scraping_db;
   ```

2. **RabbitMQ**: Debe estar ejecutÃ¡ndose en otro proyecto con:
   - Usuario: `admin`
   - ContraseÃ±a: `admin123`

3. **La tabla se crearÃ¡ automÃ¡ticamente** cuando ejecutes el worker por primera vez.

## ğŸš€ Uso

### GuÃ­a de EjecuciÃ³n Paso a Paso

#### Paso 1: PreparaciÃ³n del Entorno
```bash
# 1. Crear entorno virtual
python -m venv venv

# 2. Activar entorno virtual
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Configurar variables de entorno
copy config.env.example .env
# Editar .env con tus credenciales
```

#### Paso 2: Verificar Conexiones
```bash
# Probar que SQL Server y RabbitMQ estÃ©n funcionando
python test_connection.py
```

#### Paso 3: Iniciar el Worker
```bash
# En una terminal, ejecutar el worker
python main.py
```

El worker se conectarÃ¡ a RabbitMQ y SQL Server, y comenzarÃ¡ a procesar tareas de scraping.

#### Paso 4: Publicar Tareas de Scraping

#### Tarea Ãšnica
```bash
python publisher.py --url "https://ejemplo.com" --selenium
```

#### MÃºltiples URLs desde archivo
```bash
# Crear archivo urls.txt con URLs (una por lÃ­nea)
echo "https://ejemplo1.com" > urls.txt
echo "https://ejemplo2.com" >> urls.txt

# Publicar tareas
python publisher.py --file urls.txt --selenium
```

#### Con Selectores CSS Personalizados
```bash
# Crear archivo selectors.json
{
    "titulo": "h1.title",
    "contenido": "div.content",
    "fecha": "span.date"
}

# Publicar con selectores
python publisher.py --url "https://ejemplo.com" --selectors selectors.json
```

### ğŸ¯ EjecuciÃ³n en ProducciÃ³n
```bash
# Ejecutar worker de producciÃ³n (SIEMPRE ACTIVO)
python run_production_worker.py
```

### ğŸ”„ Flujo de Trabajo del Sistema

1. **RecepciÃ³n**: El worker recibe mensajes de aseguradoras desde RabbitMQ
2. **Procesamiento**: Extrae el `NombreCompleto` del mensaje
3. **BÃºsqueda**: Consulta la tabla `urls_automatizacion` en SQL Server
4. **CachÃ©**: Almacena URLs en memoria para futuras consultas
5. **Resultado**: Combina informaciÃ³n del mensaje con la URL encontrada

### ğŸ“‹ Comandos de Ejemplo

```bash
# Iniciar worker de producciÃ³n (SIEMPRE ACTIVO)
python run_production_worker.py

# El worker estarÃ¡ siempre esperando mensajes
# Presiona Ctrl+C para detenerlo de forma graceful
```

## ğŸ“Š Estructura de la Base de Datos

### Tabla: urls_automatizacion

| Campo | Tipo | DescripciÃ³n |
|-------|------|-------------|
| id | UNIQUEIDENTIFIER | ID Ãºnico de la aseguradora |
| nombre | NVARCHAR(255) | Nombre completo de la aseguradora |
| url_login | TEXT | URL de login de la aseguradora |
| url_destino | TEXT | URL de destino (opcional) |
| descripcion | NTEXT | DescripciÃ³n de la aseguradora |
| fecha_creacion | DATETIME | Fecha de creaciÃ³n del registro |

### Formato de Mensajes RabbitMQ

Los mensajes deben contener el campo `NombreCompleto` que se usarÃ¡ para buscar en la tabla:

```json
{
    "NombreCompleto": "PAN AMERICAN LIFE DE ECUADOR",
    "IdFactura": "FACT001",
    "IdAseguradora": 14,
    "NumDocIdentidad": "1234567890",
    "PersonaPrimerNombre": "JUAN",
    "PersonaPrimerApellido": "PEREZ",
    "FechaProcesamiento": "2025-01-09T15:00:00Z"
}
```

## ğŸ“ CaracterÃ­sticas del Worker

### ğŸš€ Modo SIEMPRE ACTIVO
- El worker estÃ¡ configurado para estar **siempre esperando** mensajes
- No se cierra cuando la cola estÃ¡ vacÃ­a
- Procesa mensajes de forma continua y asÃ­ncrona

### ğŸ’¾ Sistema de CachÃ©
- Almacena URLs de aseguradoras en memoria
- Evita consultas repetidas a la base de datos
- Mejora significativamente el rendimiento

### ğŸ”„ Procesamiento Robusto
- Manejo de errores sin interrumpir el worker
- Acknowledgment manual de mensajes
- ReconexiÃ³n automÃ¡tica en caso de fallos

### ğŸ“Š Logging Detallado
- Logs en consola y archivo (`production_worker.log`)
- EstadÃ­sticas del cachÃ© al iniciar y detener
- InformaciÃ³n detallada de cada mensaje procesado

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno

El worker lee la configuraciÃ³n desde el archivo `.env`:

```bash
# SQL Server
SQL_SERVER_HOST=localhost\MSSQLSERVER01
SQL_SERVER_DATABASE=NeptunoMedicalAutomatico
SQL_SERVER_TRUSTED_CONNECTION=yes

# RabbitMQ
RABBITMQ_HOST=localhost
RABBITMQ_PORT=5672
RABBITMQ_USERNAME=admin
RABBITMQ_PASSWORD=admin123
RABBITMQ_QUEUE=aseguradora_queue
RABBITMQ_EXCHANGE=aseguradora_exchange
RABBITMQ_ROUTING_KEY=aseguradora

# AplicaciÃ³n
LOG_LEVEL=INFO
SCRAPING_DELAY=1
MAX_RETRIES=3
```

### Selenium vs Requests

- **Requests/BeautifulSoup**: MÃ¡s rÃ¡pido, ideal para pÃ¡ginas estÃ¡ticas
- **Selenium**: Necesario para pÃ¡ginas con JavaScript dinÃ¡mico

## ğŸ“ Logs

Los logs se guardan en:
- **Consola**: Salida en tiempo real
- **Archivo**: `scraping_worker.log`

## ğŸ› SoluciÃ³n de Problemas

### Error de ConexiÃ³n a SQL Server
- Verificar que SQL Server estÃ© ejecutÃ¡ndose en `localhost\MSSQLSERVER01`
- Comprobar que la autenticaciÃ³n de Windows estÃ© habilitada
- Asegurar que el usuario de Windows tenga permisos en la base de datos
- Verificar que el driver ODBC estÃ© instalado
- Para Docker: usar `host.docker.internal\MSSQLSERVER01`

### Error de ConexiÃ³n a RabbitMQ
- Verificar que RabbitMQ estÃ© ejecutÃ¡ndose
- Comprobar credenciales en `.env`
- Verificar que el puerto 5672 estÃ© abierto

### Error de Selenium
- Instalar Chrome/Chromium
- Verificar que el ChromeDriver estÃ© en el PATH
- Para headless, asegurar que no haya problemas de permisos

## ğŸ³ Uso con Docker

### Ejecutar Solo la AplicaciÃ³n Python
```bash
# Construir y ejecutar la aplicaciÃ³n
docker-compose up -d

# Ver logs de la aplicaciÃ³n
docker-compose logs -f scraping_app

# Detener la aplicaciÃ³n
docker-compose down
```

### ConfiguraciÃ³n para Servicios Externos
La aplicaciÃ³n se conecta a servicios externos (SQL Server y RabbitMQ) que deben estar ejecutÃ¡ndose en otros proyectos. AsegÃºrate de que:

1. **SQL Server** estÃ© ejecutÃ¡ndose y accesible
2. **RabbitMQ** estÃ© ejecutÃ¡ndose con usuario `admin` y contraseÃ±a `admin123`
3. Los hosts en el archivo `.env` apunten a los servicios correctos

### Variables de Entorno para Docker
```bash
# Crear archivo .env con las configuraciones de tus servicios externos
cp docker.env.example .env

# Editar .env con tus configuraciones:
SQL_SERVER_HOST=host.docker.internal\MSSQLSERVER01
SQL_SERVER_TRUSTED_CONNECTION=yes
RABBITMQ_HOST=tu_rabbitmq_host
```

**Nota**: Para Docker, usa `host.docker.internal\MSSQLSERVER01` para conectar al SQL Server del host con autenticaciÃ³n de Windows.

### Conectar a Red Externa
Si tus servicios externos estÃ¡n en otra red Docker, puedes conectar la aplicaciÃ³n:

```yaml
# En docker-compose.yml, descomenta y configura:
networks:
  - external_network

# Luego ejecutar:
docker network connect external_network scraping_app
```

## ğŸ”„ Escalabilidad

Para ejecutar mÃºltiples workers:

1. **Ejecutar mÃºltiples instancias** del worker en diferentes terminales
2. **Usar Docker** para containerizar la aplicaciÃ³n
3. **Implementar balanceo de carga** con mÃºltiples servidores

## ğŸ“ˆ Monitoreo

### Estado de la Cola
```python
from src.scraping_worker import ScrapingWorker

worker = ScrapingWorker()
worker.initialize()
status = worker.get_queue_status()
print(f"Mensajes en cola: {status['message_count']}")
```

### Consultas SQL Ãštiles

```sql
-- Ãšltimos resultados
SELECT TOP 10 * FROM scraping_results ORDER BY timestamp DESC;

-- EstadÃ­sticas por dÃ­a
SELECT 
    CAST(timestamp AS DATE) as fecha,
    COUNT(*) as total,
    AVG(processing_time) as tiempo_promedio
FROM scraping_results 
GROUP BY CAST(timestamp AS DATE)
ORDER BY fecha DESC;

-- URLs con errores
SELECT url, error_message, timestamp 
FROM scraping_results 
WHERE error_message IS NOT NULL;
```

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crear una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abrir un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

## ğŸ†˜ Soporte

Si tienes problemas o preguntas:

1. Revisar los logs en `scraping_worker.log`
2. Verificar la configuraciÃ³n en `.env`
3. Comprobar que todos los servicios estÃ©n ejecutÃ¡ndose
4. Abrir un issue en el repositorio

## ğŸ“‹ Resumen de Comandos Principales

| Comando | DescripciÃ³n |
|---------|-------------|
| `python test_connection.py` | Probar conexiones a SQL Server y RabbitMQ |
| `python main.py` | Iniciar el worker de scraping |
| `python publisher.py --url "URL"` | Publicar tarea Ãºnica |
| `python publisher.py --file archivo.txt` | Publicar mÃºltiples URLs |
| `python quick_start.py` | Prueba rÃ¡pida completa del sistema |
| `docker-compose up -d` | Iniciar todo con Docker |

## ğŸ¯ Casos de Uso TÃ­picos

### Escenario 1: Prueba Inicial
```bash
# 1. Asegurar que SQL Server y RabbitMQ estÃ©n ejecutÃ¡ndose en otros proyectos
# 2. Configurar .env con las credenciales correctas
# 3. Probar conexiones
python test_connection.py

# 4. Ejecutar prueba rÃ¡pida
python quick_start.py
```

### Escenario 2: Scraping de Sitio Web
```bash
# 1. Iniciar worker
python main.py

# 2. Publicar tarea con selectores
python publisher.py --url "https://ejemplo.com" --selectors selectors.json
```

### Escenario 3: Procesamiento Masivo
```bash
# 1. Crear archivo con URLs
echo "https://sitio1.com" > urls.txt
echo "https://sitio2.com" >> urls.txt

# 2. Iniciar worker
python main.py

# 3. Publicar tareas masivas
python publisher.py --file urls.txt --selenium
```

### Escenario 4: EjecuciÃ³n con Docker
```bash
# 1. Configurar .env con servicios externos
# 2. Ejecutar con Docker
docker-compose up -d

# 3. Ver logs
docker-compose logs -f scraping_app
```

## ğŸ”„ Cambios Implementados

### 1. MigraciÃ³n de Chrome a Microsoft Edge
- **Cambio de navegador**: Migrado de Google Chrome a Microsoft Edge para mejor estabilidad en Windows
- **Selenium actualizado**: Actualizado de Selenium 3.141.0 a Selenium 4.35.0 para compatibilidad con Edge
- **Mejor rendimiento**: Edge suele ser mÃ¡s estable y compatible con Selenium en entornos Windows
- **ConfiguraciÃ³n optimizada**: Opciones especÃ­ficas de Edge para reducir logs y mejorar rendimiento

### 2. Aumento de Tiempos de Espera OAuth2
- **Primera redirecciÃ³n**: Aumentada de 60 segundos (20 intentos Ã— 3s) a **120 segundos (40 intentos Ã— 3s)**
- **Segunda redirecciÃ³n**: Aumentada de 30 segundos (10 intentos Ã— 3s) a **60 segundos (20 intentos Ã— 3s)**

### 2. DetecciÃ³n y Manejo de PÃ¡gina `authorization.ping`
- **DetecciÃ³n automÃ¡tica**: El sistema ahora detecta cuando se queda en la pÃ¡gina intermedia `authorization.ping`
- **BÃºsqueda activa de elementos**: Busca botones, enlaces y elementos clickeables para continuar el flujo OAuth2
- **MÃºltiples selectores**: Busca en `button`, `input[type="submit"]`, `a`, `[role="button"]`, `.btn`, `.button`
- **Logging detallado**: Registra todos los elementos encontrados y los intentos de clic

### 3. NavegaciÃ³n Manual como Fallback
- **RedirecciÃ³n automÃ¡tica**: Si el flujo OAuth2 no llega a `benefitsdirect.palig.com`, se intenta navegaciÃ³n manual
- **URL objetivo**: `https://benefitsdirect.palig.com/Inicio/Contenido/InfoAsegurado/MisPolizasPVR.aspx`
- **VerificaciÃ³n**: Se confirma que la navegaciÃ³n manual fue exitosa

### 4. Mejoras en NavegaciÃ³n a PÃ¡gina de BÃºsqueda
- **VerificaciÃ³n de estado**: DespuÃ©s del login, se verifica si ya estamos en la pÃ¡gina correcta
- **NavegaciÃ³n directa**: Si no estamos en la pÃ¡gina correcta, se intenta navegaciÃ³n directa
- **Reintentos**: Si falla, se reintenta la navegaciÃ³n
- **VerificaciÃ³n de carga**: Se espera explÃ­citamente a que `document.readyState == "complete"`

### 5. Manejo de RedirecciÃ³n a PÃ¡gina Principal
- **DetecciÃ³n**: Se detecta cuando el sistema aterriza en la pÃ¡gina principal en lugar de la de bÃºsqueda
- **MÃºltiples estrategias de redirecciÃ³n**:
  - **Estrategia 1**: NavegaciÃ³n directa a la URL objetivo
  - **Estrategia 2**: BÃºsqueda y clic en enlaces relevantes en la pÃ¡gina principal
  - **Estrategia 3**: NavegaciÃ³n a URLs alternativas como Ãºltimo recurso
- **Logging detallado**: Se registra cada estrategia y su resultado
- **Fallback inteligente**: Si una estrategia falla, se intenta la siguiente

### 6. Sistema de Logging Detallado de URLs ğŸ†•
- **Rastreo de cambios de URL**: Se detecta y registra cada cambio de URL durante el flujo OAuth2
- **Logs antes y despuÃ©s**: Se registra la URL antes y despuÃ©s de cada navegaciÃ³n
- **DetecciÃ³n de estados intermedios**: Se identifica claramente cuando se estÃ¡ en `authorization.ping`
- **Seguimiento de estrategias**: Cada estrategia de navegaciÃ³n registra su URL objetivo y resultado
- **VerificaciÃ³n de resultados**: Se confirma la URL final despuÃ©s de cada operaciÃ³n
- **Logs estructurados**: Formato consistente con emojis y jerarquÃ­a visual para fÃ¡cil lectura

## ğŸ§ª Scripts de Prueba

### `test_oauth2_flow.py`
Script independiente para probar solo el flujo OAuth2 mejorado:
```bash
python test_oauth2_flow.py
```

### `test_complete_flow.py`
Script independiente para probar el flujo completo incluyendo navegaciÃ³n post-login:
```bash
python test_complete_flow.py
```

### `test_url_logging.py` ğŸ†•
Script independiente para verificar el sistema de logging de URLs:
```bash
python test_url_logging.py
```

## ğŸ“ Notas de ImplementaciÃ³n

### Logging de URLs
El sistema ahora registra detalladamente:
- **URL inicial** antes de cada operaciÃ³n
- **URL despuÃ©s** de cada operaciÃ³n
- **Cambios detectados** durante el flujo OAuth2
- **Resultado de cada estrategia** de navegaciÃ³n
- **URLs finales** despuÃ©s de cada proceso

### Formato de Logs
Los logs utilizan un formato estructurado con:
- Emojis para identificaciÃ³n visual rÃ¡pida
- JerarquÃ­a clara con indentaciÃ³n
- InformaciÃ³n de URL antes y despuÃ©s
- Estados y transiciones claramente marcados

### Beneficios del Nuevo Sistema
1. **Trazabilidad completa**: Se puede seguir exactamente el flujo de navegaciÃ³n
2. **Debugging mejorado**: IdentificaciÃ³n rÃ¡pida de dÃ³nde se queda el proceso
3. **Control de flujo**: Visibilidad total de las redirecciones OAuth2
4. **AnÃ¡lisis de fallos**: FÃ¡cil identificaciÃ³n de quÃ© estrategia fallÃ³
5. **Monitoreo en tiempo real**: Seguimiento del progreso durante la ejecuciÃ³n